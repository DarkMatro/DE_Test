# Проект: Автоматизация обработки логов CRUD-операций

## Описание
Этот проект представляет собой систему для автоматизированной пакетной обработки логов CRUD-операций пользователей. В рамках проекта используется **Apache Airflow**, который по расписанию ежедневно в 7:00 запускает процесс вычисления агрегатов данных за последние 7 дней.

Для ускорения пересчётов проект включает механизм кеширования промежуточных результатов, что позволяет избежать повторной обработки больших объёмов данных.

### Основные компоненты проекта:
- **Docker**: Система развернута в контейнерах с использованием Docker Compose.
- **Apache Airflow**: Используется для автоматизации и планирования задач.
- **Polars**: Для высокопроизводительной и эффективной обработки данных.

## Файлы проекта:
- **`docker-compose.yml`** — файл для настройки и запуска сервисов через Docker Compose.
- **`spark_aggregator_dag.py`** — DAG для Airflow, который ежедневно в 7:00 запускает обработку данных.
- **`script.py`** — основной скрипт обработки логов CRUD-операций с кешированием промежуточных результатов.
- **`spark_job.py`** — альтернативный скрипт для обработки данных, который использует Polars.

## Используемые технологии:
- **Apache Airflow**: Автоматизация и планирование задач.
- **Polars**: Высокопроизводительная библиотека для обработки данных в Python.
- **Docker**: Контейнеризация среды выполнения.
- **Python**: Основной язык программирования для обработки данных.

## Установка и запуск

### 1. Настройка проекта
Склонируйте репозиторий и перейдите в папку проекта:

```bash
git clone https://github.com/DarkMatro/DE_Test
cd <project-directory>
```

### 2. Запуск Docker контейнеров
Для развертывания всех необходимых сервисов используйте Docker Compose. Убедитесь, что Docker и Docker Compose установлены на вашей машине.

Запуск контейнеров:
```bash
docker-compose up -d
```
Это создаст и запустит контейнеры с Airflow, PostgreSQL и другими необходимыми компонентами.

### 3. Использование Apache Airflow
После успешного запуска контейнеров, интерфейс Apache Airflow будет доступен по адресу: http://localhost:8080
Для входа используйте следующие учетные данные: 
Логин: admin
Пароль: admin

### 4. Структура DAG в Airflow
DAG (Directed Acyclic Graph) настроен для ежедневного выполнения в 7:00 утра. Он использует задачу BashOperator для запуска скрипта, который агрегирует данные за последние 7 дней.

### 5. Кеширование данных
Каждый день данные за предыдущие 7 дней обрабатываются и сохраняются в кеш в формате Parquet для последующего использования. Это помогает ускорить пересчеты и избежать повторной обработки данных, которые уже были обработаны ранее.

## Пример использования
Для запуска агрегирования данных вручную:
```bash
python script.py <input_dir> <output_dir> <target_date>
python script.py ./input ./output 2024-09-24
```
## Параметры:
- **input_dir** — директория с логами в формате CSV.
- **output_dir** — директория для сохранения агрегированных данных.
- **target_date** — дата, на которую нужно произвести расчет (например, 2024-09-24).

## Структура данных
Логи содержат следующие поля:
- email — электронная почта пользователя.
- action — одно из действий: CREATE, READ, UPDATE, DELETE.
- timestamp — дата и время действия

Результаты вычисления агрегатов сохраняются в формате CSV с колонками:
- email
- create_count
- read_count
- update_count
- delete_count